{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SviEavTyDQYh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import itertools\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zI2gqLqVEdmN"
   },
   "outputs": [],
   "source": [
    "# This works\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(num_features, num_features, 3),\n",
    "                      nn.InstanceNorm2d(num_features),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(num_features, num_features, 3),\n",
    "                      nn.InstanceNorm2d(num_features)]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pIQUv-4DGVjk"
   },
   "outputs": [],
   "source": [
    "# Generator is correct\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution block       \n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(3, 64, 7),\n",
    "                 nn.InstanceNorm2d(64),\n",
    "                 nn.ReLU(inplace=True)]\n",
    "\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                        nn.InstanceNorm2d(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(9):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model += [nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                      nn.InstanceNorm2d(out_features),\n",
    "                      nn.ReLU(inplace=True)]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.ReflectionPad2d(3),\n",
    "                  nn.Conv2d(64, 3, 7),\n",
    "                  nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_ILc1n6GlLx"
   },
   "outputs": [],
   "source": [
    "# Discriminator is good \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        model = [nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "                 nn.InstanceNorm2d(128), \n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "                 nn.InstanceNorm2d(256), \n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Conv2d(256, 512, 4, padding=1),\n",
    "                 nn.InstanceNorm2d(512), \n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Conv2d(512, 1, 4, padding=1),\n",
    "                 nn.Sigmoid()\n",
    "                ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  self.model(x)\n",
    "#         return x\n",
    "        # Average pooling and flatten\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjy5b-hVyHxh"
   },
   "outputs": [],
   "source": [
    "class FakePool():\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def query(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0,1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size-1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7mq02zVG7v8"
   },
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self, batch_size=1, test_batch_size=1,\n",
    "            epochs=10, lr=0.001, momentum=0.5,seed=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size # Input batch size for testing\n",
    "        self.epochs = epochs # Number of epochs to train\n",
    "        self.lr = lr # Learning rate\n",
    "        self.momentum = momentum\n",
    "        self.seed = seed # Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yycMpqxjMEET"
   },
   "outputs": [],
   "source": [
    "# This time, there is no label. Just input 2 different images\n",
    "class PainterDataset():\n",
    "    def __init__(self, img_dir, transform=None, unaligned=False, mode='train'):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.unaligned = unaligned\n",
    "        \n",
    "        pathA = os.path.join(self.img_dir, mode+'A')\n",
    "        pathB = os.path.join(self.img_dir, mode+'B')\n",
    "        \n",
    "        self.filesA = sorted(glob.glob(pathA + '/*.*'))\n",
    "        self.filesB = sorted(glob.glob(pathB + '/*.*'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filesA)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        files_A = self.filesA[idx % len(self.filesA)]\n",
    "        img1 = Image.open(files_A)\n",
    "        \n",
    "        indexB = -1\n",
    "        if self.unaligned:\n",
    "            indexB = random.randint(0, len(self.filesB) -1)\n",
    "        else:\n",
    "            indexB = idx % len(self.filesB)\n",
    "        files_B = self.filesB[indexB]\n",
    "        img2 = Image.open(files_B)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            \n",
    "        return img1, img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQqIvkq9L8U1"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(object):\n",
    "    data_dir = 'datasets/nouveau2roman/'\n",
    "    transforms_ = transforms.Compose([ transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ])\n",
    "    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    train_dataset = PainterDataset(img_dir = data_dir, transform = transforms_, unaligned=True)\n",
    "    test_dataset = PainterDataset(img_dir = data_dir, transform = transforms_, mode='test')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YwornxfUIlz3"
   },
   "outputs": [],
   "source": [
    "def train(args, train_loader):  \n",
    "    # Define models\n",
    "    netG_A2B = Generator()\n",
    "    netG_B2A = Generator()\n",
    "    netD_A = Discriminator()\n",
    "    netD_B = Discriminator()\n",
    "    \n",
    "    netG_A2B.cuda()\n",
    "    netG_B2A.cuda()\n",
    "    netD_A.cuda()\n",
    "    netD_B.cuda()\n",
    "    \n",
    "    # May want to init weight here\n",
    "    # .apply(weights_init_normal?)\n",
    "    \n",
    "    # Define Loss function\n",
    "    criterion_GAN = nn.BCELoss()\n",
    "    criterion_cycle = nn.L1Loss()\n",
    "    criterion_identity = nn.L1Loss()\n",
    "    \n",
    "    # Optimizer and decreasing LR\n",
    "    optimizer_G = optim.Adam(\n",
    "        itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
    "        lr=args.lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    optimizer_D_A = torch.optim.Adam(netD_A.parameters(), \n",
    "                                     lr=args.lr, betas=(0.5, 0.999))\n",
    "    optimizer_D_B = torch.optim.Adam(netD_B.parameters(), \n",
    "                                     lr=args.lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    target_real = Variable(torch.Tensor(args.batch_size).fill_(1.0).cuda(), requires_grad=False)\n",
    "    target_fake = Variable(torch.Tensor(args.batch_size).fill_(0.0).cuda(), requires_grad=False)\n",
    "    \n",
    "    fakeA_buffer = FakePool()\n",
    "    fakeB_buffer = FakePool()\n",
    "    \n",
    "    # May want to have lr_scheduler here\n",
    "    \n",
    "    epochs_to_run = args.epochs\n",
    "    progress_bar = tqdm.tqdm(train_loader, desc='Training')\n",
    "\n",
    "    \n",
    "    for epoch in range (1, 1 + epochs_to_run):\n",
    "        for batch_idx, (inputA, inputB) in enumerate(progress_bar):\n",
    "            # Input\n",
    "            inputA, inputB = Variable(inputA.cuda()), Variable(inputB.cuda())\n",
    "            \n",
    "            # Forward pass\n",
    "            fakeB = netG_A2B(inputA) \n",
    "            recoveredA = netG_B2A(fakeB)\n",
    "            fakeA = netG_B2A(inputB)\n",
    "            recoveredB = netG_A2B(fakeA)\n",
    "            \n",
    "            # Backward for Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Identity Loss\n",
    "            sameA = netG_B2A(inputA)\n",
    "            loss_identity_A = criterion_identity(sameA, inputA)*5.0\n",
    "            \n",
    "            sameB = netG_A2B(inputB)\n",
    "            loss_identity_B = criterion_identity(sameB, inputB)*5.0\n",
    "            \n",
    "            # GAN Loss\n",
    "            pred_fake = netD_B(fakeB)\n",
    "            loss_GAN_A2B = criterion_GAN(pred_fake, target_real)\n",
    "            \n",
    "            pred_fake = netD_A(fakeA)\n",
    "            loss_GAN_B2A = criterion_GAN(pred_fake, target_real)\n",
    "            \n",
    "            # Cycle Loss\n",
    "            loss_cycle_ABA = criterion_cycle(recoveredA, inputA)*10.0\n",
    "            loss_cycle_BAB = criterion_cycle(recoveredB, inputB)*10.0\n",
    "            \n",
    "            # Total Loss\n",
    "            loss = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # Discriminator A backward\n",
    "            optimizer_D_A.zero_grad()\n",
    "            \n",
    "            # Real loss\n",
    "            pred_real = netD_A(inputA)\n",
    "            loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "            \n",
    "            # Fake loss\n",
    "            fakeA = fakeA_buffer.query(fakeA)\n",
    "            pred_fake = netD_A(fakeA.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "            \n",
    "            # Total loss\n",
    "            loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
    "            loss_D_A.backward()\n",
    "            \n",
    "            optimizer_D_A.step()\n",
    "            \n",
    "            # Disciminator B backward\n",
    "            optimizer_D_B.zero_grad()\n",
    "            \n",
    "            # Real loss\n",
    "            pred_real = netD_B(inputB)\n",
    "            loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "            \n",
    "            # Fake loss\n",
    "            fakeB = fakeB_buffer.query(fakeB)\n",
    "            pred_fake = netD_B(fakeB.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "            \n",
    "            # Total loss\n",
    "            loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
    "            loss_D_B.backward()\n",
    "            \n",
    "            optimizer_D_B.step()\n",
    "            \n",
    "            \n",
    "            progress_bar.set_description(\n",
    "            'Epoch: {} Generator loss: {} Discriminator A loss: {} Discriminator B loss: {}' .format(\n",
    "                epoch, loss.data[0], loss_D_A.data[0], loss_D_B.data[0]))\n",
    "        \n",
    "        progress_bar.refresh()\n",
    "        data_dir = 'datasets/nouveau2roman/'\n",
    "        # Save models\n",
    "        output_dir = os.path.join(data_dir, 'output')\n",
    "        torch.save(netG_A2B.state_dict(), os.path.join(output_dir,'netG_A2B.pth'))\n",
    "        torch.save(netG_B2A.state_dict(), os.path.join(output_dir,'netG_B2A.pth'))\n",
    "        torch.save(netD_A.state_dict(), os.path.join(output_dir,'netD_A.pth'))\n",
    "        torch.save(netD_B.state_dict(), os.path.join(output_dir,'netD_B.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0QZqo-cL2BE"
   },
   "outputs": [],
   "source": [
    "def test(args, test_loader):\n",
    "    # Define models\n",
    "    netG_A2B = Generator()\n",
    "    netG_B2A = Generator()\n",
    "    \n",
    "    netG_A2B.cuda()\n",
    "    netG_B2A.cuda()\n",
    "    \n",
    "    # Load in the model\n",
    "    data_dir = 'datasets/nouveau2roman/'\n",
    "    output_dir = os.path.join(data_dir, 'output')\n",
    "    netG_A2B.load_state_dict(torch.load(os.path.join(output_dir,'netG_A2B.pth')))\n",
    "    netG_B2A.load_state_dict(torch.load(os.path.join(output_dir,'netG_B2A.pth')))\n",
    "    \n",
    "    # Test\n",
    "    netG_A2B.eval()\n",
    "    netG_B2A.eval()\n",
    "    \n",
    "    progress_bar = tqdm.tqdm(test_loader, desc='Validation')\n",
    "    \n",
    "    for index, (inputA, inputB) in enumerate(progress_bar):\n",
    "        # Input\n",
    "        inputA, inputB = Variable(inputA.cuda()), Variable(inputB.cuda())\n",
    "        \n",
    "        # Generate output\n",
    "        with torch.no_grad():\n",
    "            outB = netG_A2B(inputA)\n",
    "            outA = netG_B2A(inputB)\n",
    "        \n",
    "        # Save files\n",
    "        save_image(outA, os.path.join(output_dir,'A/%04d.png' %(index+1)))\n",
    "        save_image(outB, os.path.join(output_dir,'B/%04d.png' %(index+1)))\n",
    "        \n",
    "        print('Generated images %04d of %04d' %(index+1, len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "yVC4_wfIL3E_",
    "outputId": "a0cf0ef6-7222-4abd-8f4f-1f1422c3e185"
   },
   "outputs": [],
   "source": [
    "data_dir = 'datasets/nouveau2roman/'\n",
    "args = Args()\n",
    "train_loader, test_loader = prepare_dataset(args)\n",
    "\n",
    "train(args, train_loader)\n",
    "test(args, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9lUEd4lWmm-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
